{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6dff7b8-5c31-4211-a7ef-d690dfea5b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T20:24:27.266065Z",
     "iopub.status.busy": "2025-08-17T20:24:27.265627Z",
     "iopub.status.idle": "2025-08-17T20:30:46.375074Z",
     "shell.execute_reply": "2025-08-17T20:30:46.373918Z",
     "shell.execute_reply.started": "2025-08-17T20:24:27.266034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Creating/Verifying Training Script and Dependencies ---\n",
      "✅ Fresh 'train_lora.py' (using TRL) and 'requirements.txt' created successfully.\n",
      "\n",
      "--- Step 4: Setting up Session and AWS Clients ---\n",
      "✅ Session established. Using role: AmazonSageMaker-ExecutionRole-20250817T092438\n",
      "\n",
      "--- Step 5: Securely Fetching Hugging Face Token ---\n",
      "✅ Successfully fetched Hugging Face token from Secrets Manager.\n",
      "\n",
      "--- Step 6: Defining Buckets and Hyperparameters ---\n",
      "✅ Hyperparameters defined. Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\n",
      "--- Step 7: Configuring the SageMaker Training Job ---\n",
      "✅ SageMaker Estimator created.\n",
      "\n",
      "--- Step 8: Launching the Training Job ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2025-08-17-20-24-27-463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 20:24:28 Starting - Starting the training job\n",
      "2025-08-17 20:24:28 Pending - Training job waiting for capacity......\n",
      "2025-08-17 20:25:06 Pending - Preparing the instances for training...\n",
      "2025-08-17 20:25:50 Downloading - Downloading the training image..................\n",
      "2025-08-17 20:28:57 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:17,166 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:17,184 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:17,194 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:17,195 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:18,636 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.44.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (4.36.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (0.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 2)) (0.25.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 5)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 5)) (0.8.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (3.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 2)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 2)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r requirements.txt (line 2)) (2024.9.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r requirements.txt (line 2)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 5)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 5)) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 5)) (3.10.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (2.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (6.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r requirements.txt (line 2)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 5)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 5)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r requirements.txt (line 2)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 5)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 5)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 5)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 5)) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl->-r requirements.txt (line 5)) (0.2.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.2 -> 25.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:19,972 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:19,972 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,011 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,039 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,067 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,078 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "        \"per_device_train_batch_size\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2025-08-17-20-24-27-463\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edenred-llm-artifacts-ab-20250817/huggingface-pytorch-training-2025-08-17-20-24-27-463/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_lora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train_lora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"learning_rate\":0.0002,\"model_id\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"per_device_train_batch_size\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_lora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_lora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edenred-llm-artifacts-ab-20250817/huggingface-pytorch-training-2025-08-17-20-24-27-463/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"learning_rate\":0.0002,\"model_id\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"per_device_train_batch_size\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2025-08-17-20-24-27-463\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edenred-llm-artifacts-ab-20250817/huggingface-pytorch-training-2025-08-17-20-24-27-463/source/sourcedir.tar.gz\",\"module_name\":\"train_lora\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train_lora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--learning_rate\",\"0.0002\",\"--model_id\",\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"--per_device_train_batch_size\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train_lora.py --dataset_path /opt/ml/input/data/training --epochs 1 --learning_rate 0.0002 --model_id TinyLlama/TinyLlama-1.1B-Chat-v1.0 --per_device_train_batch_size 1\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,078 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:20,078 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 128 examples [00:00, 53393.43 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 128/128 [00:00<00:00, 12033.69 examples/s]\u001b[0m\n",
      "\u001b[34mStarting fine-tuning with SFTTrainer...\u001b[0m\n",
      "\u001b[34m0%|          | 0/32 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m3%|▎         | 1/32 [00:00<00:26,  1.17it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 2/32 [00:01<00:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 3/32 [00:01<00:17,  1.64it/s]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 4/32 [00:02<00:16,  1.72it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 5/32 [00:03<00:15,  1.78it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 6/32 [00:03<00:14,  1.81it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 7/32 [00:04<00:13,  1.83it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 8/32 [00:04<00:12,  1.85it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 9/32 [00:05<00:12,  1.86it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 10/32 [00:05<00:11,  1.86it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 3.0941, 'learning_rate': 0.00014375, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 10/32 [00:05<00:11,  1.86it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 11/32 [00:06<00:11,  1.87it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 12/32 [00:06<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 13/32 [00:07<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 14/32 [00:07<00:09,  1.87it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 15/32 [00:08<00:09,  1.87it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 16/32 [00:08<00:08,  1.88it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 17/32 [00:09<00:08,  1.87it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 18/32 [00:09<00:07,  1.88it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 19/32 [00:10<00:06,  1.88it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 20/32 [00:11<00:06,  1.87it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4426, 'learning_rate': 8.125000000000001e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 20/32 [00:11<00:06,  1.87it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 21/32 [00:11<00:05,  1.87it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 22/32 [00:12<00:05,  1.87it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 23/32 [00:12<00:04,  1.87it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 24/32 [00:13<00:04,  1.87it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 25/32 [00:13<00:03,  1.87it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 26/32 [00:14<00:03,  1.88it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 27/32 [00:14<00:02,  1.88it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 28/32 [00:15<00:02,  1.87it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 29/32 [00:15<00:01,  1.87it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 30/32 [00:16<00:01,  1.87it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1208, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 30/32 [00:16<00:01,  1.87it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 31/32 [00:16<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 32/32 [00:17<00:00,  1.87it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 17.542, 'train_samples_per_second': 7.297, 'train_steps_per_second': 1.824, 'train_loss': 2.532963901758194, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 32/32 [00:17<00:00,  1.87it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 32/32 [00:17<00:00,  1.82it/s]\u001b[0m\n",
      "\u001b[34m🎉 Fine-tuning complete!\u001b[0m\n",
      "\u001b[34mSaving final LoRA adapters to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:49,448 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:49,448 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-08-17 20:29:49,449 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-08-17 20:30:21 Uploading - Uploading generated training model\n",
      "2025-08-17 20:30:21 Completed - Training job completed\n",
      "Training seconds: 286\n",
      "Billable seconds: 286\n",
      "\n",
      "--- 🎉 Training Job Submitted Successfully! ---\n"
     ]
    }
   ],
   "source": [
    "# This script should be run in a SageMaker Studio Notebook cell.\n",
    "# FINAL VERSION: This script programmatically creates a robust training script\n",
    "# using the industry-standard TRL library's SFTTrainer, which solves data collation issues.\n",
    "# It also securely fetches the Hugging Face token and launches the training job.\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Step 1: Define the Training Script Content using TRL's SFTTrainer ---\n",
    "# This is a more robust approach that handles data processing complexities automatically.\n",
    "\n",
    "script_content = \"\"\"\n",
    "# Final Corrected Script for Fine-Tuning using TRL SFTTrainer\n",
    "import argparse, os, torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "# *** THE FIX IS HERE: Added TrainingArguments to the import statement ***\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer # Using the Supervised Fine-tuning Trainer from TRL\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    # This function now returns a list of formatted strings.\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Instruction: {example['instruction'][i]}\\\\n### Output: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def main(args):\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "    # Load dataset\n",
    "    raw_dataset = load_dataset(\"json\", data_files=os.path.join(args.dataset_path, \"instructions.jsonl\"), split=\"train\")\n",
    "\n",
    "    # Model and tokenizer loading\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True, token=hf_token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True, token=hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "    # SFTTrainer handles all the complexities of data collation, padding, and labels.\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=raw_dataset,\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=formatting_prompts_func, # Pass our formatting function\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=args.output_dir,\n",
    "            per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_train_epochs=args.epochs,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            fp16=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"Starting fine-tuning with SFTTrainer...\")\n",
    "    trainer.train()\n",
    "    print(\"🎉 Fine-tuning complete!\")\n",
    "    \n",
    "    print(f\"Saving final LoRA adapters to {args.output_dir}\")\n",
    "    trainer.save_model(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_id\", type=str)\n",
    "    parser.add_argument(\"--dataset_path\", type=str)\n",
    "    parser.add_argument(\"--epochs\", type=int)\n",
    "    parser.add_argument(\"--per_device_train_batch_size\", type=int)\n",
    "    parser.add_argument(\"--learning_rate\", type=float)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"/opt/ml/model\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 2: Define Dependencies for the new script ---\n",
    "requirements_content = \"\"\"\n",
    "peft\n",
    "bitsandbytes\n",
    "accelerate\n",
    "trl\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Step 3: Creating/Verifying Training Script and Dependencies ---\")\n",
    "os.makedirs('src', exist_ok=True)\n",
    "with open('src/train_lora.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(script_content)\n",
    "with open('src/requirements.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements_content)\n",
    "print(\"✅ Fresh 'train_lora.py' (using TRL) and 'requirements.txt' created successfully.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 4: Setting up Session and AWS Clients ---\")\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "role_name = role.split('/')[-1]\n",
    "secrets_client = boto3.client('secretsmanager')\n",
    "print(f\"✅ Session established. Using role: {role_name}\")\n",
    "\n",
    "print(\"\\n--- Step 5: Securely Fetching Hugging Face Token ---\")\n",
    "secret_name = \"huggingface-access-token\"\n",
    "hf_token = \"\"\n",
    "try:\n",
    "    response = secrets_client.get_secret_value(SecretId=secret_name)\n",
    "    hf_token = json.loads(response['SecretString'])['hf_token']\n",
    "    print(\"✅ Successfully fetched Hugging Face token from Secrets Manager.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not fetch token. This is likely an IAM permission issue.\")\n",
    "    print(f\"   MANUAL ACTION REQUIRED: Please go to the IAM console, find the role '{role_name}',\")\n",
    "    print(f\"   and attach an inline policy that allows 'secretsmanager:GetSecretValue' on the secret '{secret_name}'.\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 6: Defining Buckets and Hyperparameters ---\")\n",
    "input_bucket = 'edenred-invoice-data-ab-20250817'\n",
    "output_bucket = 'edenred-llm-artifacts-ab-20250817'\n",
    "training_data_uri = f's3://{input_bucket}/instructions/'\n",
    "\n",
    "hyperparameters = {\n",
    "    'model_id': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    'dataset_path': '/opt/ml/input/data/training',\n",
    "    'epochs': 1,\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'learning_rate': 2e-4,\n",
    "}\n",
    "print(f\"✅ Hyperparameters defined. Using model: {hyperparameters['model_id']}\")\n",
    "\n",
    "print(\"\\n--- Step 7: Configuring the SageMaker Training Job ---\")\n",
    "environment = {\n",
    "    'HF_TOKEN': hf_token\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train_lora.py',\n",
    "    source_dir='./src',\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.36',\n",
    "    pytorch_version='2.1',\n",
    "    py_version='py310',\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment=environment,\n",
    "    output_path=f's3://{output_bucket}/',\n",
    ")\n",
    "print(\"✅ SageMaker Estimator created.\")\n",
    "\n",
    "print(\"\\n--- Step 8: Launching the Training Job ---\")\n",
    "if hf_token:\n",
    "    huggingface_estimator.fit({'training': training_data_uri})\n",
    "    print(\"\\n--- 🎉 Training Job Submitted Successfully! ---\")\n",
    "else:\n",
    "    print(\"\\n--- 🛑 Training job not started due to missing token. Please fix IAM permissions and re-run. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6b5e748-fbca-4596-adab-968a996195ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T03:31:38.898085Z",
     "iopub.status.busy": "2025-08-18T03:31:38.897814Z",
     "iopub.status.idle": "2025-08-18T03:31:43.913075Z",
     "shell.execute_reply": "2025-08-18T03:31:43.912368Z",
     "shell.execute_reply.started": "2025-08-18T03:31:38.898065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created code/inference.py\n",
      "✅ Created code/requirements.txt\n",
      "🚀 Creating HuggingFace model...\n",
      "🚀 Deploying with CPU instance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://edenred-llm-artifacts-ab-20250817/huggingface-pytorch-training-2025-08-17-20-24-27-463/output/model.tar.gz), script artifact (code), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-453553127570/huggingface-pytorch-inference-2025-08-18-03-31-38-967/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2025-08-18-03-31-42-185\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-cpu-1755487898\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-cpu-1755487898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deployment initiated!\n",
      "🎯 Endpoint Name: huggingface-cpu-1755487898\n",
      "⏳ Deployment will take 5-10 minutes...\n",
      "\n",
      "======================================================================\n",
      "🔗 COPY THIS TO YOUR LAMBDA ENVIRONMENT VARIABLE:\n",
      "SAGEMAKER_ENDPOINT_NAME = huggingface-cpu-1755487898\n",
      "======================================================================\n",
      "\n",
      "🧪 Testing endpoint (waiting for deployment to complete)...\n",
      "⚠️ Test failed (endpoint might still be deploying): An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint huggingface-cpu-1755487898 of account 453553127570 not found.\n",
      "💡 Try testing again in a few minutes\n"
     ]
    }
   ],
   "source": [
    "# Run this in your SageMaker notebook to fix the deployment\n",
    "\n",
    "import os\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# Step 1: Create the code directory and inference script\n",
    "os.makedirs('code', exist_ok=True)\n",
    "\n",
    "# Step 2: Create a simple inference.py script\n",
    "inference_code = '''\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir,\n",
    "            torch_dtype=torch.float32,  # Use float32 for CPU compatibility\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        return {\"model\": model, \"tokenizer\": tokenizer}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        input_data = json.loads(request_body)\n",
    "        return input_data\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Generate prediction\"\"\"\n",
    "    try:\n",
    "        model = model_dict[\"model\"]\n",
    "        tokenizer = model_dict[\"tokenizer\"]\n",
    "        \n",
    "        # Extract inputs and parameters\n",
    "        inputs = input_data.get(\"inputs\", \"\")\n",
    "        parameters = input_data.get(\"parameters\", {})\n",
    "        \n",
    "        # Default parameters\n",
    "        max_new_tokens = parameters.get(\"max_new_tokens\", 100)\n",
    "        temperature = parameters.get(\"temperature\", 0.7)\n",
    "        do_sample = parameters.get(\"do_sample\", True)\n",
    "        \n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer.encode(inputs, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            \"generated_text\": generated_text,\n",
    "            \"input_length\": len(input_ids[0]),\n",
    "            \"output_length\": len(output_ids[0])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        return {\"error\": str(e), \"generated_text\": \"Sorry, I encountered an error generating a response.\"}\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format the output\"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        return json.dumps(prediction), accept\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported accept type: {accept}\")\n",
    "'''\n",
    "\n",
    "# Write the inference script\n",
    "with open('code/inference.py', 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(\"✅ Created code/inference.py\")\n",
    "\n",
    "# Step 3: Create requirements.txt for the inference environment\n",
    "requirements = '''\n",
    "transformers==4.37.0\n",
    "torch>=2.0.0\n",
    "accelerate\n",
    "'''\n",
    "\n",
    "with open('code/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"✅ Created code/requirements.txt\")\n",
    "\n",
    "# Step 4: Deploy the model with CPU instance (avoids GPU driver issues)\n",
    "print(\"🚀 Creating HuggingFace model...\")\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=\"s3://edenred-llm-artifacts-ab-20250817/huggingface-pytorch-training-2025-08-17-20-24-27-463/output/model.tar.gz\",\n",
    "    role=\"arn:aws:iam::453553127570:role/service-role/AmazonSageMaker-ExecutionRole-20250817T092438\",\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\"  # Now this directory exists!\n",
    ")\n",
    "\n",
    "print(\"🚀 Deploying with CPU instance...\")\n",
    "\n",
    "# Generate unique endpoint name\n",
    "endpoint_name = f'huggingface-cpu-{int(time.time())}'\n",
    "\n",
    "try:\n",
    "    llm_predictor = huggingface_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',  # CPU instance - no GPU driver issues\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=False  # Don't wait for deployment to complete\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Deployment initiated!\")\n",
    "    print(f\"🎯 Endpoint Name: {endpoint_name}\")\n",
    "    print(\"⏳ Deployment will take 5-10 minutes...\")\n",
    "    \n",
    "    # Print the endpoint name to copy to Lambda\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔗 COPY THIS TO YOUR LAMBDA ENVIRONMENT VARIABLE:\")\n",
    "    print(f\"SAGEMAKER_ENDPOINT_NAME = {endpoint_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test the endpoint (this will wait for deployment)\n",
    "    print(\"\\n🧪 Testing endpoint (waiting for deployment to complete)...\")\n",
    "    \n",
    "    test_data = {\n",
    "        \"inputs\": \"Hello, how are you?\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 50,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = llm_predictor.predict(test_data)\n",
    "        print(\"✅ Test successful!\")\n",
    "        print(f\"📄 Response: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Test failed (endpoint might still be deploying): {e}\")\n",
    "        print(\"💡 Try testing again in a few minutes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Deployment failed: {e}\")\n",
    "    print(\"💡 Try running the deployment again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f8057-e7f2-4635-a354-f57af2499394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
